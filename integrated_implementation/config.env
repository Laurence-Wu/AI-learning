# BERT Comparison Training Configuration

# Model Configuration
VOCAB_SIZE=30522
# Reduced for memory constraints and faster training
HIDDEN_SIZE=384
NUM_HIDDEN_LAYERS=6  
NUM_ATTENTION_HEADS=6
INTERMEDIATE_SIZE=1536
MAX_POSITION_EMBEDDINGS=512

# Training Configuration
BATCH_SIZE=16
LEARNING_RATE=1e-4
NUM_EPOCHS=100
MAX_SEQ_LENGTH=256
MLM_PROBABILITY=0.15
WARMUP_STEPS=200
LOGGING_STEPS=20
EVAL_STEPS=100
SAVE_STEPS=1000
GRADIENT_ACCUMULATION_STEPS=4

# Training Options
FP16=true
SEED=42

# Data Configuration
TRAINING_DATA_FILE=training_data.txt
TRAIN_SPLIT=0.8

# Output Configuration
OUTPUT_DIR=./bert_comparison_outputs
PLOT_SAVE_PATH=bert_comparison.png

# Attention Algorithm Comparison List
# Available options: standard, rope, exposb, absolute
ATTENTION_ALGORITHMS=standard,rope,exposb,absolute

# Model save paths for each attention type
STANDARD_MODEL_SAVE_PATH=bert_standard_attention.pt
ROPE_MODEL_SAVE_PATH=bert_rope_attention.pt
EXPOSB_MODEL_SAVE_PATH=bert_exposb_attention.pt
ABSOLUTE_MODEL_SAVE_PATH=bert_absolute_attention.pt

# Device Configuration
DEVICE=auto