# BERT Comparison Training Configuration
# ===========================================
# Updated with GPU-compatible settings for CUDA/cuDNN compatibility
#
# GPU COMPATIBILITY NOTES:
# - Reduced model size (256 hidden, 4 layers) to avoid memory issues
# - Disabled FP16 to prevent cuDNN frontend errors
# - Conservative memory usage (70% instead of 90%)
# - Smaller batch size (4) and sequence length (128)
# - CLM training now supports all attention mechanisms with causal masking
#
# ENVIRONMENT SETUP (run before training):
# source ~/triton-env-stable/bin/activate
# export LD_LIBRARY_PATH="/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
# export CUDA_HOME=/usr/local/cuda
# export CUDA_LAUNCH_BLOCKING=1
# export TORCH_USE_CUDA_DSA=1

# Experiment Configuration
EXPERIMENT_NAME=bert_comparison
DESCRIPTION=Comprehensive BERT attention mechanism comparison with GPU optimization

# Training Objectives (mlm, clm, both)
OBJECTIVES=both

# Attention Mechanisms (standard, rope, exposb, absolute)
ATTENTION_ALGORITHMS=standard,rope,exposb,absolute

# Model Configuration (8GB GPU RAM optimized)
VOCAB_SIZE=30522
HIDDEN_SIZE=192
NUM_HIDDEN_LAYERS=3
NUM_ATTENTION_HEADS=3
INTERMEDIATE_SIZE=768
MAX_POSITION_EMBEDDINGS=256
ATTENTION_DROPOUT=0.1
HIDDEN_DROPOUT=0.1

# Training Configuration (8GB GPU RAM optimized - stability focused)
BATCH_SIZE=2
LEARNING_RATE=1e-5
NUM_EPOCHS=5
MAX_SEQ_LENGTH=128
MLM_PROBABILITY=0.15
WARMUP_STEPS=50
GRADIENT_ACCUMULATION_STEPS=4
GRADIENT_CLIPPING=0.1
WEIGHT_DECAY=0.01

# Simple Training Configuration (no statistical analysis)
# All complex features removed for simple loss comparison only

# MLM Strategy Configuration (standard, dynamic, span, whole_word)
MLM_STRATEGY=standard
MIN_MLM_PROB=0.10
MAX_MLM_PROB=0.20
SPAN_LENGTH_MEAN=3.0
MAX_SPAN_LENGTH=10

# CLM Configuration
CLM_STRATEGY=standard

# Optimizer Configuration
OPTIMIZER_TYPE=adamw
OPTIMIZER_BETAS=0.9,0.999
OPTIMIZER_EPS=1e-8

# Scheduler Configuration
SCHEDULER_TYPE=cosine
SCHEDULER_WARMUP_STEPS=100

# Training Options (GPU-compatible settings)
FP16=false
SEED=42
# CUSTOM_SEEDS=42,123,456  # Not needed for simple comparison
DISTRIBUTED=false
DATALOADER_NUM_WORKERS=2

# Data Configuration
TRAINING_DATA_FILE=training_data.txt
TRAIN_SPLIT=0.8
VAL_SPLIT=0.1
TEST_SPLIT=0.1
MIN_CHARS=20
MAX_CHARS=1000
DEDUPLICATE=true

# Local Training Data (8GB GPU optimized)
# Uses training_data.txt in current directory

# Output Configuration
OUTPUT_DIR=./outputs
SAVE_CONFIG=true
SAVE_PLOTS=true
SAVE_MODELS=true
SAVE_CHECKPOINTS=false

# Logging Configuration
LOG_LEVEL=INFO
LOG_TO_FILE=true
LOG_FILE=training.log

# Memory and Performance (8GB GPU optimized)
MAX_MEMORY_USAGE=0.6
DATALOADER_PIN_MEMORY=true
DATALOADER_PERSISTENT_WORKERS=false
EMPTY_CACHE_STEPS=10
GRADIENT_CHECKPOINTING=true

# Evaluation Configuration
EVAL_STEPS=50
SAVE_STEPS=500
LOGGING_STEPS=10
EARLY_STOPPING_PATIENCE=3

# Device Configuration
DEVICE=cuda

# Debug and Development
DEBUG=false
PROFILE=false
DRY_RUN=false

# Model save paths for each attention type
STANDARD_MODEL_SAVE_PATH=bert_standard_attention.pt
ROPE_MODEL_SAVE_PATH=bert_rope_attention.pt
EXPOSB_MODEL_SAVE_PATH=bert_exposb_attention.pt
ABSOLUTE_MODEL_SAVE_PATH=bert_absolute_attention.pt