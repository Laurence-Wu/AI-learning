# Default Configuration for BERT Attention Mechanism Comparison
# =============================================================

# Experiment metadata
experiment_name: "bert_attention_comparison"
description: "Comprehensive comparison of BERT attention mechanisms with MLM and CLM training"
version: "1.0.0"
tags: ["bert", "attention", "comparison", "mlm", "clm"]

# Training objectives to compare
training_objectives: ["mlm", "clm"]

# Attention mechanisms to compare
attention_algorithms: ["standard", "rope", "exposb", "absolute"]

# Training configuration
training:
  # Basic training parameters
  num_epochs: 20
  batch_size: 32
  learning_rate: 5e-5
  warmup_steps: 1000
  
  # Regularization
  weight_decay: 0.01
  gradient_clipping: 1.0
  dropout: 0.1
  
  # Optimization
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  
  # Scheduler settings
  scheduler:
    type: "cosine"
    num_warmup_steps: 1000
    num_training_steps: null  # Calculated automatically
    min_lr_ratio: 0.01
  
  # Optimizer settings
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
    eps: 1e-8
    amsgrad: false
  
  # Logging and saving
  logging_steps: 100
  eval_steps: 500
  save_steps: 2000
  
  # Performance
  num_workers: 4
  pin_memory: true
  dataloader_drop_last: true

# Model configuration
model:
  # Architecture
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 512
  
  # Regularization
  attention_dropout: 0.1
  hidden_dropout: 0.1
  layer_norm_eps: 1e-12
  
  # Initialization
  initializer_range: 0.02
  
  # Vocabulary
  vocab_size: 30522  # BERT base vocabulary
  type_vocab_size: 2
  pad_token_id: 0

# Data configuration
data:
  # Data files
  training_data_file: "./training_data/processed_training_data.txt"
  validation_split: 0.1
  max_examples: null  # Use all available data
  
  # Sequence processing
  max_seq_length: 512
  truncation: true
  padding: "max_length"
  
  # MLM configuration
  mlm_strategy: "standard"  # standard, dynamic, span, whole_word
  mlm_probability: 0.15
  
  # MLM strategy-specific parameters
  mlm_dynamic:
    min_probability: 0.10
    max_probability: 0.20
  
  mlm_span:
    span_length_mean: 3.0
    span_length_std: 1.0
    max_span_length: 10
  
  mlm_whole_word:
    # Uses tokenizer word boundaries
    extend_probability: 0.3
  
  # CLM configuration
  clm_strategy: "standard"  # standard, packed, sliding
  
  # CLM strategy-specific parameters
  clm_packed:
    pack_sequences: true
    separator_token: "[SEP]"
  
  clm_sliding:
    window_size: 256
    stride: 128
  
  # Preprocessing
  preprocessing:
    clean_html: true
    clean_urls: true
    clean_emails: true
    normalize_unicode: true
    normalize_whitespace: true
    min_chars: 20
    max_chars: 10000
    min_words: 5
    max_words: 1000
    filter_non_ascii: false
    deduplicate: true
    dedupe_threshold: 0.9

# Resource management
device: "auto"  # auto, cpu, cuda, mps
mixed_precision: true
gradient_checkpointing: false
memory_efficient_attention: true

# Reproducibility
seed: 42
deterministic: true

# Output and logging
output_dir: "./outputs"
log_level: "INFO"
save_config: true

# Experiment tracking
use_wandb: false
wandb_project: "bert-attention-comparison"
wandb_entity: null
wandb_tags: ["bert", "attention"]

# Monitoring
monitor_gpu: true
monitor_memory: true
profile_performance: false

# Evaluation
evaluation:
  # Metrics to compute
  compute_perplexity: true
  compute_accuracy: true
  
  # Evaluation frequency
  eval_during_training: true
  eval_at_end: true
  
  # Early stopping
  early_stopping:
    enabled: false
    patience: 5
    min_delta: 0.001
    metric: "eval_loss"
    mode: "min"