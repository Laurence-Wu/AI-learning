# BERT Comparison Training Configuration - Optimized for Colab A100 GPU
# This configuration maximizes performance on A100 GPU with 40GB VRAM

# Model Configuration - Optimized for A100
VOCAB_SIZE=30522
HIDDEN_SIZE=1024
NUM_HIDDEN_LAYERS=12
NUM_ATTENTION_HEADS=16
INTERMEDIATE_SIZE=4096
MAX_POSITION_EMBEDDINGS=1024

# Training Configuration - A100 Optimized
BATCH_SIZE=64
LEARNING_RATE=2e-4
NUM_EPOCHS=100
MAX_SEQ_LENGTH=512
MLM_PROBABILITY=0.15
WARMUP_STEPS=1000
LOGGING_STEPS=50
EVAL_STEPS=200
SAVE_STEPS=2000
GRADIENT_ACCUMULATION_STEPS=2

# Training Options - A100 Specific
FP16=true
SEED=42

# Data Configuration
TRAINING_DATA_FILE=training_data.txt
TRAIN_SPLIT=0.85

# Output Configuration
OUTPUT_DIR=./bert_comparison_outputs_A100
PLOT_SAVE_PATH=bert_comparison_A100.png

# Attention Algorithm Comparison List
# Available options: standard, rope, exposb, absolute
ATTENTION_ALGORITHMS=standard,rope,exposb,absolute

# Model save paths for each attention type - A100 optimized
STANDARD_MODEL_SAVE_PATH=bert_standard_attention_A100.pt
ROPE_MODEL_SAVE_PATH=bert_rope_attention_A100.pt
EXPOSB_MODEL_SAVE_PATH=bert_exposb_attention_A100.pt
ABSOLUTE_MODEL_SAVE_PATH=bert_absolute_attention_A100.pt

# Device Configuration
DEVICE=cuda

# A100 Performance Comments:
# - Batch size 64: Takes advantage of A100's high memory bandwidth
# - Hidden size 1024: Optimal for A100's tensor cores and memory hierarchy
# - 16 attention heads: Perfect for A100's CUDA core count
# - Sequence length 512: Balances memory usage with attention quality
# - FP16: Essential for A100 tensor core acceleration
# - Gradient accumulation 2: Simulates effective batch size of 128
# - Higher learning rate: A100 can handle more aggressive training
# - More warmup steps: Better convergence with larger models
