# BERT Comparison Training Configuration - RTX 4070 8GB Optimized
# =================================================================
# Optimized for RTX 4070 8GB VRAM with CUDA compute capability 8.9
#
# RTX 4070 OPTIMIZATION NOTES:
# - Increased model size to better utilize 8GB VRAM (384 hidden, 6 layers)
# - Enabled mixed precision for RTX 4070's efficient FP16 Tensor Cores
# - Optimized batch size and sequence length for memory efficiency
# - Tuned for maximum utilization while maintaining stability
# - Fixed MLM masking ensures stable training across all attention mechanisms

# Experiment Configuration
experiment_name: "bert_attention_comparison_4070"
description: "BERT attention mechanism comparison optimized for RTX 4070 8GB"
version: "2.0.0"
tags: ["bert", "attention", "comparison", "rtx-4070", "optimized"]

# Training Objectives and Attention Mechanisms
training_objectives: ["mlm", "clm"]
attention_algorithms: ["standard", "rope", "exposb", "absolute"]

# Model Configuration (RTX 4070 8GB optimized)
model:
  vocab_size: 30522
  hidden_size: 384          # Increased from 192 to better utilize GPU
  num_hidden_layers: 6      # Increased from 3 for more meaningful comparison
  num_attention_heads: 6    # Increased from 3 (matches hidden_size/64)
  intermediate_size: 1536   # Increased from 768 (4x hidden_size standard)
  max_position_embeddings: 512  # Increased from 256 for longer sequences
  attention_dropout: 0.1
  hidden_dropout: 0.1

# Training Configuration (RTX 4070 optimized)
training:
  batch_size: 8             # Increased from 2 to utilize GPU memory
  learning_rate: 0.0001     # Increased from 1e-5 for faster convergence
  num_epochs: 10            # Increased for more comprehensive comparison
  warmup_steps: 200         # Increased proportionally
  gradient_accumulation_steps: 2  # Reduced since batch_size increased
  max_grad_norm: 1.0        # Increased from 0.5 for better gradient flow
  weight_decay: 0.01
  gradient_clipping: 1.0    # Increased from 0.5
  
  # Optimizer Configuration
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 0.01
  
  # Scheduler Configuration
  scheduler:
    type: "cosine"
    num_warmup_steps: 100   # Adjusted for new training length
  
  # Training Options (RTX 4070 optimized)
  fp16: true              # Enabled for RTX 4070 Tensor Cores
  num_workers: 4          # Increased from 2 for better CPU utilization
  pin_memory: true
  dataloader_drop_last: true
  logging_steps: 20       # Increased from 10 to reduce overhead
  eval_steps: 100         # Increased from 50
  save_steps: 1000        # Increased from 500

# Data Configuration
data:
  max_seq_length: 256       # Increased from 128 to utilize longer contexts
  mlm_probability: 0.15
  training_data_file: "training_data.txt"
  validation_split: 0.2
  min_chars: 20
  max_chars: 2000           # Increased from 1000 for richer data
  deduplicate: true
  truncation: true
  padding: "max_length"
  
  # MLM Strategy Configuration
  mlm_strategy: "standard"
  
  # CLM Configuration
  clm_strategy: "standard"

# Output Configuration
output_dir: "./outputs"
save_config: true

# Logging Configuration
log_level: "INFO"

# Memory and Performance (RTX 4070 optimized)
gradient_checkpointing: false    # Disabled for RTX 4070 - we have enough memory
mixed_precision: true           # Enabled for RTX 4070 Tensor Cores
memory_efficient_attention: true

# Device Configuration
device: "cuda"

# Reproducibility
seed: 42
deterministic: true

# Monitoring
monitor_gpu: true
monitor_memory: true
profile_performance: true        # Enabled for performance analysis

# Experiment tracking
use_wandb: false
wandb_project: "bert-attention-comparison-rtx4070"

# RTX 4070 Specific Optimizations
cuda_optimizations:
  enable_flash_attention: true   # Use Flash Attention if available
  compile_model: true           # Enable torch.compile for RTX 4070
  use_channels_last: true       # Memory layout optimization
  allow_tf32: true              # Enable TF32 for faster training