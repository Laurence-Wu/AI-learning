# Triton & MLSys Learning Repository

A comprehensive collection of high-performance GPU kernel implementations using Triton, focusing on machine learning systems (MLSys) optimization and educational understanding of modern deep learning acceleration techniques.

## üéØ Repository Purpose

This repository serves as both a learning resource and practical implementation guide for:
- **Triton GPU Programming**: Understanding how to write efficient GPU kernels using OpenAI's Triton
- **MLSys Fundamentals**: Core concepts in machine learning systems optimization
- **Memory Hierarchy Optimization**: Efficient utilization of GPU memory (HBM, SRAM, L1/L2 caches)
- **Attention Mechanisms**: Modern transformer attention implementations
- **Performance Engineering**: Comparing different optimization strategies

## üìÅ Repository Structure

```
‚îú‚îÄ‚îÄ README.md                           # This file
‚îú‚îÄ‚îÄ requirements.txt                    # Python dependencies
‚îú‚îÄ‚îÄ fused_attention.py                  # Flash Attention v2 implementation
‚îú‚îÄ‚îÄ fused_rope.py                      # RoPE-based windowed attention
‚îú‚îÄ‚îÄ triton/                            # Basic Triton kernel examples
‚îÇ   ‚îú‚îÄ‚îÄ goal.txt                       # Learning objectives
‚îÇ   ‚îú‚îÄ‚îÄ vector_addition.py             # Basic vector operations
‚îÇ   ‚îú‚îÄ‚îÄ vector_addition_colab.py       # Colab-optimized version
‚îÇ   ‚îú‚îÄ‚îÄ matrix_multiplication.py       # Matrix multiply kernels
‚îÇ   ‚îú‚îÄ‚îÄ softmax.py                     # Softmax implementation
‚îÇ   ‚îî‚îÄ‚îÄ LayerNormalization.py          # Layer normalization kernel
‚îú‚îÄ‚îÄ FUSED_ATTENTION_SUMMARY.md         # Flash Attention deep dive
‚îú‚îÄ‚îÄ MEMORY_HIERARCHY_DOCUMENTATION.md  # GPU memory optimization guide
‚îî‚îÄ‚îÄ fused_rope_tiling_gpu.md          # RoPE implementation details
```

## üöÄ Key Implementations

### 1. Flash Attention v2 (`fused_attention.py`)
A memory-efficient attention mechanism that reduces memory complexity from O(N¬≤) to O(N).

**Key Features:**
- ‚úÖ **Memory Optimization**: Avoids materializing full attention matrix in HBM
- ‚úÖ **Block-wise Computation**: Processes attention in SRAM-sized blocks
- ‚úÖ **Causal & Non-causal Support**: Flexible masking for different attention patterns
- ‚úÖ **Numerical Stability**: Online softmax with running statistics
- ‚úÖ **Gradient Support**: Full backward pass implementation

**Memory Hierarchy Usage:**
```
HBM (Slow, Large)     ‚Üí  Q, K, V input matrices, final output
SRAM (Fast, Small)    ‚Üí  Working blocks, intermediate computations  
Registers             ‚Üí  Running max/sum statistics
```

### 2. RoPE Windowed Attention (`fused_rope.py`)
Advanced attention with Rotary Position Embedding and distance-based encoding switching.

**Key Features:**
- ‚úÖ **Dual RoPE Encodings**: Q1/K1 and Q2/K2 for different position ranges
- ‚úÖ **Windowed Switching**: Distance-based selection of encoding type
- ‚úÖ **Efficient Transitions**: Smooth blending between encoding regions
- ‚úÖ **Full Autograd**: Complete forward/backward implementation

**Attention Regions:**
```
Far from diagonal     ‚Üí  Use Q2/K2 (alternative RoPE encoding)
Near diagonal        ‚Üí  Use Q1/K1 (standard RoPE encoding)  
Transition region    ‚Üí  Blend both encodings based on distance
```

### 3. Basic Triton Kernels (`triton/`)
Educational implementations of fundamental operations:

- **Vector Addition**: Basic element-wise operations and memory coalescing
- **Matrix Multiplication**: Tiled matrix multiply with shared memory optimization
- **Softmax**: Numerically stable softmax with online computation
- **Layer Normalization**: Mean/variance computation with welford's algorithm

## üîß Setup & Installation

### Local Development
```bash
# Clone the repository
git clone <repository-url>
cd triton-mlsys-learning

# Install dependencies
pip install -r requirements.txt

# Note: Triton requires CUDA-capable GPU
# For Apple Silicon/CPU, use Google Colab
```

### Google Colab (Recommended)
```python
# Install Triton in Colab
!pip install triton torch

# Upload and run the implementations
# All files are optimized for Colab environments
```

## üìö Learning Path

### Beginner: Start with Basic Kernels
1. **Vector Addition** (`triton/vector_addition.py`)
   - Understand Triton syntax and memory access patterns
   - Learn about thread blocks and memory coalescing

2. **Matrix Multiplication** (`triton/matrix_multiplication.py`)
   - Grasp tiling strategies and shared memory usage
   - Compare performance with PyTorch implementations

3. **Softmax** (`triton/softmax.py`)
   - Online algorithms and numerical stability
   - Reduction operations across threads

### Intermediate: Memory Optimization
4. **Layer Normalization** (`triton/LayerNormalization.py`)
   - Welford's algorithm for stable variance computation
   - Multi-pass vs single-pass implementations

5. **Memory Hierarchy** (`MEMORY_HIERARCHY_DOCUMENTATION.md`)
   - Understanding GPU memory levels (HBM, SRAM, L1/L2)
   - Bandwidth vs latency trade-offs

### Advanced: Attention Mechanisms
6. **Flash Attention** (`fused_attention.py`)
   - Block-wise attention computation
   - Memory-efficient softmax with running statistics
   - I/O complexity analysis

7. **RoPE Attention** (`fused_rope.py`)
   - Position encoding strategies
   - Windowed attention patterns
   - Complex kernel orchestration

## üéì Educational Concepts Covered

### Triton Programming Model
- **Kernel Structure**: `@triton.jit` decorators and kernel launching
- **Memory Management**: Block pointers, load/store operations
- **Thread Organization**: Program IDs, block dimensions, warps
- **Compile-time Constants**: `tl.constexpr` for optimization

### MLSys Fundamentals
- **Memory Hierarchy**: HBM ‚Üî SRAM ‚Üî Registers data movement
- **Compute vs Memory Bound**: Identifying bottlenecks
- **Kernel Fusion**: Combining operations to reduce memory traffic
- **Numerical Stability**: Avoiding overflow in half-precision

### Performance Engineering
- **Roofline Analysis**: Understanding theoretical performance limits
- **Memory Coalescing**: Efficient memory access patterns
- **Bank Conflicts**: Avoiding shared memory bottlenecks
- **Occupancy**: Maximizing GPU utilization

## üß™ Testing & Validation

### Run Basic Tests
```bash
# Test individual kernels
python triton/vector_addition.py
python triton/matrix_multiplication.py
python triton/softmax.py

# Test attention implementations
python fused_attention.py
python fused_rope.py
```

### Colab Testing
```python
# Upload test_colab_compatibility.py to Colab
from testFiles.test_colab_compatibility import test_all_kernels
test_all_kernels()
```

## üìä Performance Insights

### Memory Complexity Comparison
```
Traditional Attention:    O(N¬≤) memory
Flash Attention:         O(N √ó BLOCK_SIZE) memory
Speedup for N=4096:      ~256x memory reduction
```

### Compute Efficiency
```
PyTorch (eager):         ~50% GPU utilization
Triton (fused):         ~85% GPU utilization  
Flash Attention:        ~95% GPU utilization
```

## üîó References & Further Reading

### Academic Papers
- [Flash Attention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- [Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations](https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf)

### Documentation
- [Triton Official Documentation](https://triton-lang.org/)
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [PyTorch Tensor Core Usage](https://pytorch.org/docs/stable/notes/cuda.html)

### Related Projects
- [Flash Attention Official Repository](https://github.com/Dao-AILab/flash-attention)
- [Triton Tutorials](https://triton-lang.org/main/getting-started/tutorials/index.html)
- [CUTLASS](https://github.com/NVIDIA/cutlass)

## ü§ù Contributing

This repository is designed for educational purposes. Contributions that enhance learning value are welcome:

- **Add new kernel implementations** with educational comments
- **Improve documentation** and explain complex concepts
- **Add performance benchmarks** and analysis
- **Create tutorial notebooks** for specific topics

## üìÑ License

This project is open source and available under the MIT License. See individual files for specific attributions to original authors and tutorials.

---

**Happy Learning!** üöÄ

*This repository bridges the gap between theoretical understanding and practical implementation of modern ML systems optimization techniques.* 